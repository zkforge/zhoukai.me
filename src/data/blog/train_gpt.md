---
title: 如何训练一个 GPT 助手
author: Kai Zhou
pubDatetime: 2025-06-14T15:20:00.00+08:00
featured: true
draft: false
tags:
  - LLM
  - GPT
  - AI
description: 本文来自 OpenAI 的 Andrej Karpathy 在 Microsoft Build 2023 大会的分享
---

本文翻译了 Andrej Karpathy 在 Microsoft Build 2023 大会分享的第一部分内容。

## Table of contents

## 1. 引言

人工智能领域正在经历翻天覆地的变化，因此这里讲的只是到目前为止训练 GPT 助手的方法。大致分为四个阶段：

1. 预训练（Pre-training）
2. 监督微调（Supervised Fine Tuning, SFT）
3. 奖励建模（Reward Modeling）
4. 强化学习（Reinforcement Learning）

每个阶段又分为三个部分（从上到下）：数据集、算法和输出的模型。

## 2. 预训练

这个阶段占了整个过程（四个阶段）绝大部分算力，例如占据了 99% 的训练计算时间和浮点运算。

处理的是互联网规模的数据集，使用数千个 GPU 训练，可能需要数月的时间。其他三个阶段是微调阶段，只需要使用较少的 GPU 训练几个小时或几天。

### 2.1 数据集

首先需要收集大量的数据。例如，下面是 Meta 训练 LLaMA 所用的数据集：

| 数据集 | 占比 | 迭代次数（Epochs） | 数据集大小（Disk Size） |
|:-------|:-----|:-------------------|:----------------------|
| CommonCrawl | 67.0% | 1.10 | 3.3 TB |
| C4 | 15.0% | 1.06 | 783 GB |
| Github | 4.5% | 0.64 | 328 GB |
| Wikipedia | 4.5% | 2.45 | 83 GB |
| Books | 4.5% | 2.23 | 85 GB |
| ArXiv | 2.5% | 1.06 | 92 GB |
| StackExchange | 2.0% | 1.03 | 78 GB |

其中 Epochs 是用 1.4T tokens 预训练时的迭代次数。用 1T tokens 预训练时也是用的这个数据集比例。

### 2.2 文本 Token 化

在实际训练这些数据之前，需要经过一个预处理步骤，即 token 化。将原始文本翻译成整数序列，后者是 GPT 的表示方式。

- 一个 token 可能是一个单词、一个词根、标点、标点+单词等等
- 每个 token 平均对应 0.75 个单词
- 所有的独立 token 组成一个词典（词汇表），典型的词典大小：10k~100k tokens
- 这种文本/token 转换是无损的，有很多算法，例如常用的字节对编码

### 2.3 超参数：GPT-3 vs. LLaMA

接下来需要考虑控制阶段的超参数。这里拿两个具体模型 GPT-3/LLaMA 作为例子：

- GPT-4 的训练信息公开比较少，所以这里使用 GPT-3 的数据，注意 GPT-3 已经是三年前的模型了
- LLaMA 是 Meta 最近发布的一个开源模型，数据比较新，信息比较全

#### 词汇表大小、上下文长度、参数数量

预训练处理的数量级大致如下：

- 词汇表大小通常为 10K 个 token
- 上下文长度通常为 2k/4k，有时甚至 100k。这决定了 GPT 在预测序列中下一个整数时所能查看的最大整数数量

#### 硬件环境和成本

| 模型 | GPU | 训练时长 | 训练成本 |
|:-----|:----|:---------|:---------|
| GPT-3 | 约一万张 V100 | 30 天左右 | $100 万 ~ $1000 万 |
| LLaMA | 两千张 A100 | 21 天 | $500 万 |

### 2.4 开始训练

#### 根据 Batch Size 和上下文长度 Token 化输入文本

输入给 Transformer 的是 (B,T) 维度的矩阵，其中：

- B 表示批次大小（batch size）
- T 表示最大上下文长度

另外，输入会整理成行（row），每个输入序列的结尾用一个特殊的 `<|endoftext|>` token 来标记。

#### 预测下一个 Token

在预测每个位置的下一个 token 时，只能用到当前行中当前位置前面的最多 T（上下文长度）个 token。

#### 损失函数

训练一段时间之后，你会发现 Transformer 已经学会了单词以及在哪里放空格和逗号等等。因此，随着时间的推移，我们可以得到越来越一致的预测。

## 3. 监督微调（SFT）

### 3.1 收集高质量人工标注数据

在监督微调阶段，首先需要收集小但高质量的数据集。通常是通过供应商的形式收集，格式是"提示 + 优质回答"。

### 3.2 SFT 训练

然后在这些数据上再次进行语言建模：

- 算法还是一样，只是更换了训练集
- 预训练是互联网文档，这是数量庞大但质量较低的数据
- 现在是 QA 类型的提示回答类数据，数量不多但质量很高
- 这个阶段只需要百来片 GPU，训练几天时间

## 4. 奖励建模

RLHF 包括奖励建模和强化学习。在奖励建模阶段，会将数据收集转变为比较（comparison）的形式。

### 4.1 例子：评估 ChatGPT 编程的好坏

基于上一步已经训练好的 SFT 模型，让它写一个检查给定字符串是否为回文的程序或函数。我们重复三次，每次都给完全相同的提示，得到三个回答。

### 4.2 奖励

现在来看一下如何对奖励进行建模：

- 蓝色的是提示（prompt tokens），每行都一样
- 黄色的是 SFT 模型基于 prompt 产生的补全（completion tokens），每次都不同
- 绿色的是特殊的 `<|reward|>` token

### 4.3 奖励模型的特点

跟基座模型、SFT 模型以及后面将介绍的强化学习模型相比，奖励模型的最大特点是不能独立部署。

## 5. 强化学习（RLHF）

### 5.1 RLHF 训练

现在我们获取了一大批提示，接下来基于奖励模型进行强化学习：

- 针对给定提示的每个补全，奖励模型能够预测这些补全的质量
- 评分过程（或称损失函数）其实也是根据给定的一串 tokens（SFT 模型的输出）来预测下一个 token（分数）

### 5.2 为什么要使用 RLHF？

简单回答是：效果好。从人类的反馈来看，质量从高到低依次为：RLHF 模型、SFT 模型、基座模型。

### 5.3 模型的熵

某些情况下，RLHF 模型并不是基础模型的简单改进。特别是，我们注意到 RLHF 模型会丢失一些熵：

- 这意味着它们会给出更加确定性的结果
- 相比基础模型，RLHF 模型的输出变化更少
- 基础模型熵比较大，会给出很多不同的输出

## 6. 总结

目前市面上可用的助手模型中，最好的模型是 GPT-4。前三个都是 RLHF 模型，其他的都是 SFT 模型。
   